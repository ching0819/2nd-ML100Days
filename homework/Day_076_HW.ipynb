{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業重點:\n",
    "\n",
    "(1)以, Adam, 為例, 調整 batch_size, epoch , 觀察accurancy, loss 的變化\n",
    "\n",
    "(2)以同一模型, 分別驗證 SGD, Adam, Rmsprop 的 accurancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業目標:\n",
    "    \n",
    "    取得各種優化器的運算結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Blas GEMM launch failed , 避免動態分配GPU / CPU, 出現問題\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "   宣告並設定\n",
    "   batch_size：對總的樣本數進行分組，每組包含的樣本數量\n",
    "   epochs ：訓練次數\n",
    "   \n",
    "''' \n",
    "\n",
    "batch_size = 20\n",
    "num_classes = 10\n",
    "epochs = 40\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#tar -xvfz cifar-10-batches-py.tar.gz\n",
    "#ar xvfz cifar-10-batches-py.tar.gz\n",
    "# The data, split between train and test sets:\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    第一步：選擇模型, 順序模型是多個網絡層的線性堆疊\n",
    " \n",
    "model = Sequential()\n",
    "\n",
    "#   第二步：構建網絡層\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense( 10)) # 輸出結果是10個類別，所以維度是10   \n",
    "model.add(Activation('softmax')) # 最後一層用softmax作為激活函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters：1250858\n"
     ]
    }
   ],
   "source": [
    "# 模型建立完成後，統計參數總量\n",
    "print(\"Total Parameters：%d\" % model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 輸出模型摘要資訊\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#第三步編譯\n",
    "'''\n",
    " SGD(隨機梯度下降) - Arguments\n",
    "lr: float >= 0. Learning rate.\n",
    "momentum: float >= 0. Parameter that accelerates SGD in the relevant direction and dampens oscillations.\n",
    "decay: float >= 0. Learning rate decay over each update.\n",
    "nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "'''\n",
    "\n",
    "'''\n",
    "RMSprop- Arguments\n",
    "lr: float >= 0. Learning rate.\n",
    "rho: float >= 0.\n",
    "epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().\n",
    "decay: float >= 0. Learning rate decay over each update.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Example:\n",
    "opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "opt = keras.optimizers.SGD(lr=0.001, nesterov=True, momentum=0.95)\n",
    "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 資料正規化\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1.8629 - acc: 0.3138 - val_loss: 1.4888 - val_acc: 0.4606\n",
      "Epoch 2/40\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1.4330 - acc: 0.4786 - val_loss: 1.2419 - val_acc: 0.5537\n",
      "Epoch 3/40\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1.2599 - acc: 0.5485 - val_loss: 1.1395 - val_acc: 0.5935\n",
      "Epoch 4/40\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1.1444 - acc: 0.5904 - val_loss: 1.0223 - val_acc: 0.6423\n",
      "Epoch 5/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 1.0508 - acc: 0.6248 - val_loss: 0.9380 - val_acc: 0.6693\n",
      "Epoch 6/40\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 0.9746 - acc: 0.6540 - val_loss: 0.8776 - val_acc: 0.6949\n",
      "Epoch 7/40\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 0.9039 - acc: 0.6814 - val_loss: 0.8506 - val_acc: 0.7080\n",
      "Epoch 8/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.8481 - acc: 0.7001 - val_loss: 0.7778 - val_acc: 0.7280\n",
      "Epoch 9/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.8015 - acc: 0.7164 - val_loss: 0.7304 - val_acc: 0.7477\n",
      "Epoch 10/40\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 0.7597 - acc: 0.7315 - val_loss: 0.7374 - val_acc: 0.7477\n",
      "Epoch 11/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.7228 - acc: 0.7453 - val_loss: 0.6996 - val_acc: 0.7605\n",
      "Epoch 12/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.6894 - acc: 0.7577 - val_loss: 0.6965 - val_acc: 0.7604\n",
      "Epoch 13/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.6636 - acc: 0.7660 - val_loss: 0.6559 - val_acc: 0.7741\n",
      "Epoch 14/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.6404 - acc: 0.7735 - val_loss: 0.6409 - val_acc: 0.7787\n",
      "Epoch 15/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.6105 - acc: 0.7848 - val_loss: 0.6332 - val_acc: 0.7849\n",
      "Epoch 16/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.5871 - acc: 0.7915 - val_loss: 0.6371 - val_acc: 0.7799\n",
      "Epoch 17/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.5691 - acc: 0.7982 - val_loss: 0.6457 - val_acc: 0.7825\n",
      "Epoch 18/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.5459 - acc: 0.8080 - val_loss: 0.6103 - val_acc: 0.7915\n",
      "Epoch 19/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.5302 - acc: 0.8115 - val_loss: 0.5983 - val_acc: 0.7942\n",
      "Epoch 20/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.5062 - acc: 0.8199 - val_loss: 0.6230 - val_acc: 0.7896\n",
      "Epoch 21/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.4953 - acc: 0.8235 - val_loss: 0.6163 - val_acc: 0.7911\n",
      "Epoch 22/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.4818 - acc: 0.8291 - val_loss: 0.6149 - val_acc: 0.7908\n",
      "Epoch 23/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.4642 - acc: 0.8346 - val_loss: 0.6028 - val_acc: 0.7964\n",
      "Epoch 24/40\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 0.4534 - acc: 0.8380 - val_loss: 0.6012 - val_acc: 0.7970\n",
      "Epoch 25/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.4431 - acc: 0.8438 - val_loss: 0.6076 - val_acc: 0.7975\n",
      "Epoch 26/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.4251 - acc: 0.8484 - val_loss: 0.6114 - val_acc: 0.7892\n",
      "Epoch 27/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.4171 - acc: 0.8518 - val_loss: 0.6181 - val_acc: 0.7935\n",
      "Epoch 28/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.4049 - acc: 0.8546 - val_loss: 0.5928 - val_acc: 0.8015\n",
      "Epoch 29/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3967 - acc: 0.8590 - val_loss: 0.6231 - val_acc: 0.7990\n",
      "Epoch 30/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3882 - acc: 0.8619 - val_loss: 0.6155 - val_acc: 0.8020\n",
      "Epoch 31/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3711 - acc: 0.8679 - val_loss: 0.6173 - val_acc: 0.7976\n",
      "Epoch 32/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3657 - acc: 0.8700 - val_loss: 0.6116 - val_acc: 0.8045\n",
      "Epoch 33/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3575 - acc: 0.8735 - val_loss: 0.6273 - val_acc: 0.7988\n",
      "Epoch 34/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3486 - acc: 0.8763 - val_loss: 0.6054 - val_acc: 0.8059\n",
      "Epoch 35/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3433 - acc: 0.8763 - val_loss: 0.6105 - val_acc: 0.8066\n",
      "Epoch 36/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3272 - acc: 0.8838 - val_loss: 0.6151 - val_acc: 0.8075\n",
      "Epoch 37/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3247 - acc: 0.8831 - val_loss: 0.6069 - val_acc: 0.8041\n",
      "Epoch 38/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3189 - acc: 0.8861 - val_loss: 0.6037 - val_acc: 0.8070\n",
      "Epoch 39/40\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 0.3191 - acc: 0.8867 - val_loss: 0.6191 - val_acc: 0.8053\n",
      "Epoch 40/40\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 0.3088 - acc: 0.8882 - val_loss: 0.6092 - val_acc: 0.8124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n   第四步：訓練\\n   .fit的一些參數\\n   batch_size：對總的樣本數進行分組，每組包含的樣本數量\\n   epochs ：訓練次數\\n   shuffle：是否把數據隨機打亂之後再進行訓練\\n   validation_split：拿出百分之多少用來做交叉驗證\\n   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 是否要做資料處理\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history=model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    print('')\n",
    "        \n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    history=model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)   \n",
    "\n",
    "'''\n",
    "   第四步：訓練\n",
    "   .fit的一些參數\n",
    "   batch_size：對總的樣本數進行分組，每組包含的樣本數量\n",
    "   epochs ：訓練次數\n",
    "   shuffle：是否把數據隨機打亂之後再進行訓練\n",
    "   validation_split：拿出百分之多少用來做交叉驗證\n",
    "   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\n",
    "''' \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9c345826275e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# Save model and weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    第六步：輸出\n",
    "import numpy \n",
    "\n",
    "print ( \" test set \" )\n",
    "scores = model.evaluate(x_test,y_test,batch_size=200,verbose= 0)\n",
    "print ( \"\" )\n",
    "#print ( \" The test loss is %f \" % scores)\n",
    "print ( \" The test loss is %f \", scores)\n",
    "\n",
    "\n",
    "result = model.predict(x_test,batch_size=200,verbose= 0)\n",
    "\n",
    "result_max = numpy.argmax(result, axis = 1 )\n",
    "test_max = numpy.argmax(y_test, axis = 1 )\n",
    "\n",
    "result_bool = numpy.equal(result_max, test_max)\n",
    "true_num = numpy.sum(result_bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Valiidation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Valiidation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
